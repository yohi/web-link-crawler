# 要件定義書

## 概要

この文書は、ウェブサイトのURLを入力として受け取り、そのサイト内のリンクを再帰的に探索して包括的なリストを出力するウェブクローラースクリプトの要件を定義します。システムは指定されたドメイン内のページを効率的にクロールし、発見されたすべてのリンクを構造化された形式で出力します。クローラーは、ドメイン境界を尊重し、エラーを適切に処理し、設定可能な出力形式を提供するPythonコマンドラインツールとして実装されます。

## 要件

### 要件1

**ユーザーストーリー:** ユーザーとして、ウェブサイトのURLを入力し、そのサイトで見つかったすべてのリンクを再帰的に取得したい。これにより、サイト構造を分析し、利用可能なすべてのページを発見できるようになる。

#### 受け入れ基準

1. 有効なURLが提供された場合、システムはそのURLからクロールを開始する
2. ページをクロールする際、システムはアンカータグからすべてのhrefリンクを抽出する
3. リンクが見つかった場合、システムは同じドメインに属する場合にクロールキューに追加する
4. すべてのページがクロールされた場合、システムは発見されたURLの完全なリストを出力する

### 要件2

**ユーザーストーリー:** ユーザーとして、クローラーが指定されたURLのパス以下のみをクロールしてほしい。これにより、指定されたパスより上位や同レベルの他のパスを不必要にクロールしないようになる。

#### 受け入れ基準

1. 発見されたリンクを分析する際、システムは開始URLと同じドメインに属するかどうかを確認する
2. リンクが外部ドメインの場合、システムはそれを記録するが、それ以上クロールしない
3. リンクが同一ドメインでも開始URLのパスより上位の場合、システムはそれを記録するが、それ以上クロールしない
4. リンクが同一ドメインでも開始URLのパスと同レベルの他のパスの場合、システムはそれを記録するが、それ以上クロールしない
5. リンクが開始URLのパス以下の場合のみ、システムはそれをクロールキューに追加する
6. パス境界を決定する際、システムはURLパスの階層構造を正確に解析する

### 要件3

**ユーザーストーリー:** ユーザーとして、無限ループと重複処理を避けたい。これにより、クローラーが効率的に完了するようになる。

#### 受け入れ基準

1. URLに遭遇した場合、システムはそれが既に処理されているかどうかを確認する
2. URLが処理済みの場合、システムは重複を避けるためにそれをスキップする
3. クロール深度が過度になった場合、システムは設定可能な制限を持つ
4. 循環参照が存在する場合、システムはそれらを適切に処理する

### 要件4

**ユーザーストーリー:** ユーザーとして、出力が適切にフォーマットされ、情報豊富であってほしい。これにより、サイト構造を簡単に理解できるようになる。

#### 受け入れ基準

1. 結果を出力する際、システムはURLを読みやすい形式で表示する
2. 結果を表示する際、システムは各URLの深度レベルを示す
3. クロールが完了した場合、システムは要約統計を表示する
4. エラーが発生した場合、システムはプロセス全体を停止することなく、それらを明確に報告する

### 要件5

**ユーザーストーリー:** ユーザーとして、クローラーがさまざまなエッジケースを適切に処理してほしい。これにより、異なるウェブサイト間で確実に動作するようになる。

#### 受け入れ基準

1. HTTPエラーに遭遇した場合、システムはエラーをログに記録し、他のURLで続行する
2. 不正なHTMLを解析する際、システムは可能な限り抽出し、続行する
3. ネットワークタイムアウトが発生した場合、システムは指数バックオフで再試行する
4. レート制限が必要な場合、システムはリクエスト間に遅延を実装する

### 要件6

**ユーザーストーリー:** ユーザーとして、クローラーの動作を設定したい。これにより、異なる使用ケースに適応できるようになる。

#### 受け入れ基準

1. クローラーを開始する際、システムは設定用のコマンドライン引数を受け入れる
2. 最大深度を設定する際、システムは指定されたクロール深度制限を尊重する
3. 遅延を設定する際、システムはリクエスト間に指定された時間待機する
4. 出力形式を設定する際、システムは異なる出力形式（テキスト、JSON、CSV）をサポートする
5. ヘルプ情報を提供する際、システムは使用方法の説明と利用可能なオプションを表示する